{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e4ddcb",
   "metadata": {},
   "source": [
    "# Construct training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85781b1",
   "metadata": {},
   "source": [
    "## Construct preference DPO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Build ONE DPO pair per id based on reward scores.\n",
    "\n",
    "For each id:\n",
    "  - good_answer = generation with the highest reward for that id (must be > GOOD_THRESHOLD)\n",
    "  - bad_answer  = generation with the lowest  reward for that id (must be < BAD_THRESHOLD)\n",
    "\n",
    "Optionally skip zero-reward generations (SKIP_REWARD_ZERO=True).\n",
    "No similarity filtering is applied.\n",
    "\n",
    "Input\n",
    "-----\n",
    "Expects JSONL files where each line is a dict containing:\n",
    "  - \"id\"\n",
    "  - \"premises\" / \"premise\" / \"context\" (any of these)\n",
    "  - \"question\"\n",
    "  - \"generations\": list[dict], each with:\n",
    "        - \"steps\" / \"final_tag\" / \"answer\" (reasoning + final label)\n",
    "        - \"reward\" (float)\n",
    "    OR a top-level \"reward_score\": list[float] aligned with \"generations\".\n",
    "\n",
    "Configurable via `input_files`, thresholds, etc.\n",
    "\n",
    "Output\n",
    "------\n",
    "JSONL at `output_path`, one line per pair:\n",
    "\n",
    "{\n",
    "  \"id\": str,\n",
    "  \"question\": str,        # collapsed prompt (system + user + assistant prefix)\n",
    "  \"good_answer\": str,     # formatted reasoning + final answer\n",
    "  \"bad_answer\": str,      # formatted reasoning + final answer\n",
    "  \"reward\": [good, bad]   # [float, float]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ---------------------- Configure here ---------------------- #\n",
    "input_files = [\n",
    "    \"./reward_data/Multi-Thread/output/rewarded_data.jsonl\",\n",
    "    \"./refine_data/Multi-Thread/output/refined_data.jsonl\",\n",
    "]\n",
    "\n",
    "SKIP_REWARD_ZERO = True\n",
    "GOOD_THRESHOLD = 0.5\n",
    "BAD_THRESHOLD = 0.5\n",
    "\n",
    "output_path = \"./dpo_data.jsonl\"\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load a JSONL file into a list of dicts, skipping malformed lines.\"\"\"\n",
    "    recs: List[Dict[str, Any]] = []\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"[WARN] Skipping missing file: {path}\")\n",
    "        return recs\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                recs.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] Skipping bad JSON line in {path}: {e}\")\n",
    "    print(f\"[OK] Loaded {len(recs)} records from {path}\")\n",
    "    return recs\n",
    "\n",
    "\n",
    "# ---------- tiny text helpers ---------- #\n",
    "def flatten_tokens(x: Any) -> List[str]:\n",
    "    \"\"\"Recursively flatten nested token-like structures into a list of strings.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        out: List[str] = []\n",
    "        for y in x:\n",
    "            out.extend(flatten_tokens(y))\n",
    "        return out\n",
    "    if isinstance(x, str):\n",
    "        return [x]\n",
    "    return [str(x)]\n",
    "\n",
    "\n",
    "def to_text(field: Any) -> str:\n",
    "    \"\"\"Convert a token-like structure into a clean, single string.\"\"\"\n",
    "    s = \" \".join(flatten_tokens(field)).strip()\n",
    "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", s)\n",
    "\n",
    "\n",
    "def canon(s: str) -> str:\n",
    "    \"\"\"Canonicalize a string (for dedup): strip, lower, collapse whitespace.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip()).lower()\n",
    "\n",
    "\n",
    "# ---------------------- prompt building ---------------------- #\n",
    "def answer_instruction_for_id(example_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Dataset-specific answer instructions, based on the leading prefix of the id.\n",
    "    \"\"\"\n",
    "    prefix = (str(example_id).split(\"_\")[0]).lower()\n",
    "    mapping = {\n",
    "        \"folio\": (\n",
    "            \"Is the question A) True; B) False or C) Unknown based on the premises? \"\n",
    "            \"Answer with exactly one label: A, B, or C.\"\n",
    "        ),\n",
    "        \"esnli\": (\n",
    "            \"Is the question entailment/contradiction/neutral based on the premises? \"\n",
    "            \"Answer with exactly one label: entailment, contradiction, or neutral.\"\n",
    "        ),\n",
    "        \"multilogieval\": \"Answer with exactly one label: yes or no.\",\n",
    "        \"proofwriter\": (\n",
    "            \"Is the question True/False/Unknown based on the premises? \"\n",
    "            \"Answer with exactly one label: True, False, or Unknown.\"\n",
    "        ),\n",
    "        \"prontoqa\": (\n",
    "            \"Is the question True/false based on the premises? \"\n",
    "            \"Answer with exactly one label: True or False.\"\n",
    "        ),\n",
    "        \"logiqa\": (\n",
    "            \"Is the question entailment/not-entailment based on the premises? \"\n",
    "            \"Answer with exactly one label: entailment or not-entailment.\"\n",
    "        ),\n",
    "        \"proverqa\": (\n",
    "            \"Is the question A) True; B) False or C) Unknown based on the premises? \"\n",
    "            \"Answer with exactly one label: A, B, or C.\"\n",
    "        ),\n",
    "        \"qasc\": \"Answer with exactly one label: A, B, C, D, E, F, G, or H.\",\n",
    "        \"ar\": \"Answer with exactly one label: A, B, C, D, or E.\",\n",
    "        \"ar_lsat\": \"Answer with exactly one label: A, B, C, D, or E.\",\n",
    "    }\n",
    "    return mapping.get(prefix, \"\")\n",
    "\n",
    "\n",
    "def build_messages(premise: str, question: str, example_id: str):\n",
    "    \"\"\"\n",
    "    Build a (system, user) message pair encoding the step-by-step reasoning format.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a careful reasoner. Read the premise(s) and the question, then think \"\n",
    "        \"step-by-step using numbered steps. For EACH step, write three lines exactly in \"\n",
    "        \"this order and wording:\\n\"\n",
    "        \"Premise:\\n\"\n",
    "        \"Assumption:\\n\"\n",
    "        \"Conclusion:\\n\\n\"\n",
    "        \"Formatting rules:\\n\"\n",
    "        \"- Title each step as 'Step N:' where N starts at 1 and increases by 1.\\n\"\n",
    "        \"- The Premise of a step must be either (i) one of the given premises OR (ii) a Conclusion from any previous step.\\n\"\n",
    "        \"- The Assumption must be a commonsense or contextually reasonable assumption that would make sense to a human.\\n\"\n",
    "        \"- The Conclusion must be new information that logically follows from the Premise and the Assumption.\\n\"\n",
    "        \"- After you finish all steps, output a final line: 'Final answer: [xxx]'.\\n\"\n",
    "        \"- If the question has choices A), B), C) ..., put ONLY the option letter inside the brackets (e.g., [A]).\\n\"\n",
    "        \"- Otherwise, put ONLY the single required label inside the brackets (e.g., [True], [False], [entailment], [contradiction], [neutral], etc.).\\n\"\n",
    "        \"- Do NOT use XML tags. Do NOT add extra commentary before or after the steps or the final line.\\n\\n\"\n",
    "        \"-----\\n\"\n",
    "        \"Below is an example:\\n\"\n",
    "        \"Premise: Harry read a book. People who read book will be smart.\\n\"\n",
    "        \"Question: Will Harry be smart? Answer with true/false/unknown.\\n\"\n",
    "        \"Step 1:\\n\"\n",
    "        \"Premise: Harry read a book. People who read book will be smart.\\n\"\n",
    "        \"Assumption: Harry is a person. A person is people.\\n\"\n",
    "        \"Conclusion: Harry will be smart.\\n\\n\"\n",
    "        \"Final answer: [True]\\n\"\n",
    "        \"End of example\\n\"\n",
    "        \"-----\"\n",
    "    )\n",
    "    answer_sentence = answer_instruction_for_id(example_id)\n",
    "    q_with_constraint = f\"{question}\\n\\n{answer_sentence}\" if answer_sentence else question\n",
    "    user = f\"Premise:\\n{premise}\\n\\nQuestion:\\n{q_with_constraint}\\n\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "\n",
    "def collapse_messages_to_prompt(msgs: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Collapse (system, user) messages into a flat text prompt.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"system\\n\" + msgs[0][\"content\"].strip() + \"\\n\"\n",
    "        + \"user\\n\" + msgs[1][\"content\"].strip() + \"\\n\"\n",
    "        + \"assistant\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------- generation formatting ---------------------- #\n",
    "def format_generation(g: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert a rewarded generation into a text blob combining:\n",
    "      - step-by-step reasoning blocks\n",
    "      - final answer in [brackets]\n",
    "    \"\"\"\n",
    "    steps = g.get(\"steps\") or []\n",
    "    final = g.get(\"final_tag\") or g.get(\"answer\") or \"\"\n",
    "    reason_parts = []\n",
    "\n",
    "    for idx, s in enumerate(steps, 1):\n",
    "        prem_str = to_text(s.get(\"premises\") or \"\")\n",
    "        asm_str = to_text(s.get(\"assumptions\") or \"\")\n",
    "        con_str = to_text(s.get(\"conclusion\") or \"\")\n",
    "        reason_parts.append(\n",
    "            \"\\n\".join(\n",
    "                [\n",
    "                    f\"Step {idx}:\",\n",
    "                    f\"Premise: {prem_str}\",\n",
    "                    f\"Assumption: {asm_str}\",\n",
    "                    f\"Conclusion: {con_str}\",\n",
    "                ]\n",
    "            ).strip()\n",
    "        )\n",
    "\n",
    "    reasoning_str = \"\\n\\n\".join(reason_parts).strip()\n",
    "    reasoning_tagged = f\"Reasoning:\\n{reasoning_str}\" if reasoning_str else \"\"\n",
    "    answer_tagged = f\"\\nFinal answer: [{str(final).strip()}]\"\n",
    "    return (reasoning_tagged + answer_tagged).strip()\n",
    "\n",
    "\n",
    "# ---------------------- normalize inputs ---------------------- #\n",
    "def normalize_generations(rec: Dict[str, Any]) -> Tuple[str, str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Normalize a single record into:\n",
    "      - premise text\n",
    "      - question text\n",
    "      - a list of generations with fields {\"formatted\", \"reward\", \"raw\"}\n",
    "    \"\"\"\n",
    "    premise_raw = rec.get(\"premises\") or rec.get(\"premise\") or rec.get(\"context\")\n",
    "    question_raw = rec.get(\"question\") or \"\"\n",
    "\n",
    "    premise = to_text(premise_raw or \"\")\n",
    "    question = to_text(question_raw)\n",
    "    gens = rec.get(\"generations\") or []\n",
    "\n",
    "    rewards = rec.get(\"reward_score\")\n",
    "    if isinstance(rewards, list) and len(rewards) == len(gens):\n",
    "        aligned = rewards\n",
    "    else:\n",
    "        aligned = [g.get(\"reward\") for g in gens]\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for idx, g in enumerate(gens):\n",
    "        r = aligned[idx] if idx < len(aligned) else None\n",
    "        try:\n",
    "            r = float(r) if r is not None else None\n",
    "        except Exception:\n",
    "            r = None\n",
    "\n",
    "        if r is None:\n",
    "            continue\n",
    "        if SKIP_REWARD_ZERO and r == 0.0:\n",
    "            continue\n",
    "\n",
    "        formatted = format_generation(g)\n",
    "        out.append({\"formatted\": formatted, \"reward\": r, \"raw\": g})\n",
    "\n",
    "    return premise, question, out\n",
    "\n",
    "\n",
    "# ---------------------- BEST-vs-WORST with thresholds ---------------------- #\n",
    "def build_pairs(records_by_id: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build ONE DPO pair per id:\n",
    "\n",
    "      - good_answer = generation with the highest reward for that id\n",
    "                      (must be > GOOD_THRESHOLD)\n",
    "      - bad_answer  = generation with the lowest reward for that id\n",
    "                      (must be < BAD_THRESHOLD)\n",
    "\n",
    "    Skip ids that:\n",
    "      - have fewer than 2 usable generations, or\n",
    "      - do not meet the threshold constraints.\n",
    "    \"\"\"\n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total_ids = 0\n",
    "    with_pairs = 0\n",
    "\n",
    "    for _id, bundle in records_by_id.items():\n",
    "        total_ids += 1\n",
    "        premise_text = to_text(bundle.get(\"premise\", \"\"))\n",
    "        question_text = to_text(bundle.get(\"raw_question\", \"\"))\n",
    "        question = collapse_messages_to_prompt(build_messages(premise_text, question_text, _id))\n",
    "\n",
    "        gens = [\n",
    "            g\n",
    "            for g in bundle.get(\"generations\", [])\n",
    "            if isinstance(g.get(\"reward\"), (int, float))\n",
    "        ]\n",
    "        if len(gens) < 2:\n",
    "            continue\n",
    "\n",
    "        g_best = max(gens, key=lambda x: x[\"reward\"])\n",
    "        g_worst = min(gens, key=lambda x: x[\"reward\"])\n",
    "\n",
    "        # Enforce thresholds\n",
    "        if g_best[\"reward\"] <= GOOD_THRESHOLD:\n",
    "            continue\n",
    "        if g_worst[\"reward\"] >= BAD_THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        pairs.append(\n",
    "            {\n",
    "                \"id\": _id,\n",
    "                \"question\": question,\n",
    "                \"good_answer\": g_best[\"formatted\"],\n",
    "                \"bad_answer\": g_worst[\"formatted\"],\n",
    "                \"reward_good\": float(g_best[\"reward\"]),\n",
    "                \"reward_bad\": float(g_worst[\"reward\"]),\n",
    "            }\n",
    "        )\n",
    "        with_pairs += 1\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Processed ids: {total_ids} | \"\n",
    "        f\"ids with a qualifying best-vs-worst pair: {with_pairs}\"\n",
    "    )\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ---------------------- Load & Merge ---------------------- #\n",
    "all_records: List[Dict[str, Any]] = []\n",
    "for f in input_files:\n",
    "    all_records.extend(read_jsonl(f))\n",
    "\n",
    "by_id: Dict[str, Dict[str, Any]] = defaultdict(\n",
    "    lambda: {\n",
    "        \"premise\": \"\",\n",
    "        \"raw_question\": \"\",\n",
    "        \"generations\": [],\n",
    "    }\n",
    ")\n",
    "\n",
    "for rec in all_records:\n",
    "    _id = rec.get(\"id\")\n",
    "    if _id is None:\n",
    "        continue\n",
    "    premise, question, gens = normalize_generations(rec)\n",
    "\n",
    "    # Keep the longest premise / question we see for each id (best guess)\n",
    "    if len(premise) > len(by_id[_id][\"premise\"]):\n",
    "        by_id[_id][\"premise\"] = premise\n",
    "    if len(question) > len(by_id[_id][\"raw_question\"]):\n",
    "        by_id[_id][\"raw_question\"] = question\n",
    "\n",
    "    by_id[_id][\"generations\"].extend(gens)\n",
    "\n",
    "print(f\"[INFO] Unique ids merged: {len(by_id)}\")\n",
    "\n",
    "\n",
    "# ---------------------- Build ---------------------- #\n",
    "pairs = build_pairs(by_id)\n",
    "\n",
    "\n",
    "# ---------------------- Deduplicate ---------------------- #\n",
    "def dedupe_instances(pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen = set()\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for p in pairs:\n",
    "        key = (\n",
    "            p[\"id\"],\n",
    "            canon(p[\"question\"]),\n",
    "            canon(p[\"good_answer\"]),\n",
    "            canon(p[\"bad_answer\"]),\n",
    "        )\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "pairs = dedupe_instances(pairs)\n",
    "print(f\"[DEDUP] Total pairs to write: {len(pairs)}\")\n",
    "\n",
    "\n",
    "# ---------------------- Write ---------------------- #\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in pairs:\n",
    "        f.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": p[\"id\"],\n",
    "                    \"question\": p[\"question\"],\n",
    "                    \"good_answer\": p[\"good_answer\"],\n",
    "                    \"bad_answer\": p[\"bad_answer\"],\n",
    "                    \"reward\": [p[\"reward_good\"], p[\"reward_bad\"]],\n",
    "                },\n",
    "                ensure_ascii=False,\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "print(f\"[DONE] Wrote pairs to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aristotle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
