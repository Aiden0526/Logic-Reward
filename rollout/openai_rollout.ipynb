{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d054458e",
   "metadata": {},
   "source": [
    "# Use GPT-4o to rollout the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_openai_batch_requests_dedup.py\n",
    "import os, json, re, hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_JSONL  = \"./data.jsonl\"   # <-- your input\n",
    "OUT_DIR      = \"./openai_batches\"\n",
    "OPENAI_MODELS = [\"gpt-4o\"]\n",
    "N_OPENAI_SAMPLES_PER_Q = 3\n",
    "MAX_TOKENS   = 4096\n",
    "TOP_P        = 0.95\n",
    "TEMPS        = [1, 1, 1]\n",
    "BASE_SEED    = 20250815\n",
    "\n",
    "# Dedup strategy\n",
    "DEDUP_BY_KEY = True   # dedupe questions by (id, premise, question)\n",
    "# If DEDUP_BY_KEY=False, we still guarantee unique custom_id via suffixing on collision.\n",
    "\n",
    "# ---------- Robust text helpers ----------\n",
    "def flatten_tokens(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        out = []\n",
    "        for y in x:\n",
    "            out.extend(flatten_tokens(y))\n",
    "        return out\n",
    "    if isinstance(x, dict):\n",
    "        for key in (\"text\", \"content\", \"value\"):\n",
    "            if key in x and isinstance(x[key], (str, list, dict)):\n",
    "                return flatten_tokens(x[key])\n",
    "        return [json.dumps(x, ensure_ascii=False)]\n",
    "    if isinstance(x, (str, int, float, bool)):\n",
    "        return [str(x)]\n",
    "    return [str(x)]\n",
    "\n",
    "def to_text(field) -> str:\n",
    "    s = \" \".join(flatten_tokens(field))\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\")\n",
    "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# ---------- Same prompt as Qwen ----------\n",
    "def answer_instruction_for_id(example_id: str) -> str:\n",
    "    prefix = (str(example_id).split(\"_\")[0]).lower()\n",
    "    mapping = {\n",
    "        \"folio\":         \"Answer with exactly one label: True, False, or Uncertain.\",\n",
    "        \"esnli\":         \"Answer with exactly one label: entailment, contradiction, or neutral.\",\n",
    "        \"multilogieval\": \"Answer with exactly one label: yes or no.\",\n",
    "        \"proofwriter\":   \"Answer with exactly one label: True, False, or Unknown.\",\n",
    "        \"prontoqa\":      \"Answer with exactly one label: True, False, or Unknown.\",\n",
    "        \"logiqa\":        \"Answer with exactly one label: entailment or not-entailment.\",\n",
    "        \"proverqa\":      \"Answer with exactly one label: True, False, or Unknown.\",\n",
    "        \"qasc\":          \"Answer with the OPTION LETTER ONLY: A, B, C, D, E, F, G, or H.\",\n",
    "    }\n",
    "    return mapping.get(prefix, \"\")\n",
    "\n",
    "def build_messages(premise, question, example_id):\n",
    "    system = (\n",
    "        \"You are a meticulous logician. Read the premise and question carefully. \"\n",
    "        \"Reason step-by-step explicitly in a Natural Language Inference (NLI) style. \"\n",
    "        \"Each reasoning step must follow this format:\\n\\n\"\n",
    "        \"Step X:\\n\"\n",
    "        \"Premises:\\n\"\n",
    "        \"- ...\\n\"\n",
    "        \"- ...\\n\"\n",
    "        \"Assumptions:\\n\"\n",
    "        \"- ...\\n\"\n",
    "        \"Conclusion:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"Number each step (Step 1, Step 2, etc.) in order. \"\n",
    "        \"Do not skip steps; ensure each conclusion is directly supported by its premises and stated assumptions. Please make sure the assumption must make sense under the context. \\n\\n\"\n",
    "        \"Ensure that all assumptions are stated clearly and rigorously. For example, use specific entities instead of vague references such as 'it' or 'this'. Do not restate premises as assumptions. Premises must remain in the premises field, while assumptions should capture what is not directly stated in the premises but can reasonably be inferred from the premises using commonsense. Make sure all necessary information are provided in each step's premise and assumption to make the conclusion. \\n\"\n",
    "        \"After the final step, output exactly ONE final tag:\\n\"\n",
    "        \"<answer>{LABEL}</answer>\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1) If the question includes multiple-choice options labeled like 'A) ...', 'B) ...', etc., \"\n",
    "        \"   answer with the OPTION LETTER ONLY (e.g., A).\\n\"\n",
    "        \"2) Otherwise, answer with a single classification label that fits the task \"\n",
    "        \"   (e.g., True/False/Unknown, yes/no, entailment/contradiction/neutral, not-entailment, etc.).\\n\"\n",
    "        \"3) The label must match the datasetâ€™s expected surface form (case-sensitive, no extra words).\\n\"\n",
    "        \"4) After your steps, produce exactly one <answer>...</answer> tag and nothing else.\"\n",
    "    )\n",
    "    ans_sent = answer_instruction_for_id(example_id)\n",
    "    q2 = f\"{question}\\n\\n{ans_sent}\" if ans_sent else question\n",
    "    user = (\n",
    "        f\"Premise:\\n{premise}\\n\\n\"\n",
    "        f\"Question:\\n{q2}\\n\\n\"\n",
    "        \"Format your reply EXACTLY as follows:\\n\"\n",
    "        \"Step 1:\\n\"\n",
    "        \"Premises:\\n\"\n",
    "        \"- [list the premises used in this step]\\n\"\n",
    "        \"Assumptions:\\n\"\n",
    "        \"- [state any implicit assumptions made in this step]\\n\"\n",
    "        \"Conclusion:\\n\"\n",
    "        \"- [state the conclusion entailed from those premises and assumptions]\\n\\n\"\n",
    "        \"Step 2:\\n\"\n",
    "        \"Premises:\\n\"\n",
    "        \"- ...\\n\"\n",
    "        \"Assumptions:\\n\"\n",
    "        \"- ...\\n\"\n",
    "        \"Conclusion:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"...\\n\\n\"\n",
    "        \"Final:\\n\"\n",
    "        \"<answer>YOUR_SINGLE_LABEL_OR_OPTION_LETTER</answer>\\n\\n\"\n",
    "        \"Example (for just one step):\\n\"\n",
    "        \"Step 1:\\n\"\n",
    "        \"Premises:\\n\"\n",
    "        \"- All birds have wings.\\n\"\n",
    "        \"- Penguins are birds.\\n\"\n",
    "        \"Assumptions:\\n\"\n",
    "        \"- None \\n\"\n",
    "        \"Conclusion:\\n\"\n",
    "        \"- Therefore, penguins have wings.\\n\\n\"\n",
    "        \"Example (assumptions illustrated):\\n\"\n",
    "        \"Step 1:\\n\"\n",
    "        \"Premises:\\n\"\n",
    "        \"- In a school, every student is either studying in the library or playing in the playground.\\n\"\n",
    "        \"- Emily was not seen in the library on Friday.\\n\"\n",
    "        \"Assumptions:\\n\"\n",
    "        \"- Emily is a student.\\n\"\n",
    "        \"- \\\"Not seen in the library\\\" implies \\\"not studying in the library.\\\"\\n\"\n",
    "        \"Conclusion:\\n\"\n",
    "        \"- Therefore, Emily was playing in the playground on Friday.\\n\\n\"\n",
    "        \"Final:\\n\"\n",
    "        \"<answer>yes</answer> \\n\"\n",
    "        \"Make sure all necessary information are provided in each step's premise and assumption to make the conclusion.\"\n",
    "    )\n",
    "    return [{\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": user}]\n",
    "\n",
    "# ---------- Helpers for uniqueness ----------\n",
    "def key_for_rec(rec) -> str:\n",
    "    rid = rec.get(\"id\", \"\")\n",
    "    premise = to_text(rec.get(\"premise\", \"\"))\n",
    "    question = to_text(rec.get(\"question\", \"\"))\n",
    "    j = json.dumps({\"id\": rid, \"premise\": premise, \"question\": question}, ensure_ascii=False, sort_keys=True)\n",
    "    return hashlib.sha1(j.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def make_unique_custom_id(base: str, seen: set) -> str:\n",
    "    \"\"\"Return a unique custom_id by appending __dup<N> if needed.\"\"\"\n",
    "    if base not in seen:\n",
    "        seen.add(base)\n",
    "        return base\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = f\"{base}__dup{i}\"\n",
    "        if cand not in seen:\n",
    "            seen.add(cand)\n",
    "            return cand\n",
    "        i += 1\n",
    "    # Alternative: always append a global index\n",
    "    # return f\"{base}__n{len(seen)}\"\n",
    "\n",
    "# ---------- Build batch request files per model ----------\n",
    "def main():\n",
    "    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    total_requests = 0\n",
    "\n",
    "    # Optional: preload to dedupe by key\n",
    "    dedup_keys = set()\n",
    "    # For logging\n",
    "    duplicate_questions_skipped = 0\n",
    "\n",
    "    # Read the whole input once if deduping, else stream per model\n",
    "    with open(INPUT_JSONL, \"r\", encoding=\"utf-8\") as fin:\n",
    "        records = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            if DEDUP_BY_KEY:\n",
    "                k = key_for_rec(rec)\n",
    "                if k in dedup_keys:\n",
    "                    duplicate_questions_skipped += 1\n",
    "                    continue\n",
    "                dedup_keys.add(k)\n",
    "            records.append(rec)\n",
    "\n",
    "    for model in OPENAI_MODELS:\n",
    "        out_path = f\"openai_rollout_requests_{model}.jsonl\"\n",
    "        written = 0\n",
    "        seen_custom_ids = set()  # ensure no duplicates in THIS file\n",
    "\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for rec in records:\n",
    "                rid = rec.get(\"id\", \"\")\n",
    "                premise  = to_text(rec.get(\"premise\", \"\"))\n",
    "                question = to_text(rec.get(\"question\", \"\"))\n",
    "\n",
    "                messages = build_messages(premise, question, rid)\n",
    "\n",
    "                for k in range(N_OPENAI_SAMPLES_PER_Q):\n",
    "                    temp = TEMPS[k % len(TEMPS)]\n",
    "                    seed = BASE_SEED + k\n",
    "\n",
    "                    base_custom = f\"{rid}__rep{k}__{model}\"\n",
    "                    custom_id = make_unique_custom_id(base_custom, seen_custom_ids)\n",
    "\n",
    "                    req = {\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"method\": \"POST\",\n",
    "                        \"url\": \"/v1/chat/completions\",\n",
    "                        \"body\": {\n",
    "                            \"model\": model,\n",
    "                            \"messages\": messages,\n",
    "                            \"max_completion_tokens\": MAX_TOKENS,\n",
    "                            \"temperature\": 0,\n",
    "                            # \"temperature\": float(temp)\n",
    "                        },\n",
    "                    }\n",
    "                    fout.write(json.dumps(req, ensure_ascii=False) + \"\\n\")\n",
    "                    written += 1\n",
    "\n",
    "        print(f\"[{model}] Wrote {written} requests to {out_path}\")\n",
    "\n",
    "        total_requests += written\n",
    "\n",
    "    if DEDUP_BY_KEY:\n",
    "        print(f\"Skipped {duplicate_questions_skipped} duplicate questions (by (id,premise,question)).\")\n",
    "    print(f\"Total requests across models: {total_requests}\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fea8d1",
   "metadata": {},
   "source": [
    "## Helper function for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "client = openai.Client(api_key=openai.api_key)\n",
    "\n",
    "def submit_batch(batch_path, desc=\"\"):\n",
    "    path = batch_path\n",
    "    batch_input_file = client.files.create(\n",
    "    file=open(path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    print(\"Batch id: \", batch_input_file_id)\n",
    "\n",
    "    batch_request = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "        \"description\": desc\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Batch request: \", batch_request)\n",
    "\n",
    "def cancel_batch(batch_id):\n",
    "    client.batches.cancel(batch_id)\n",
    "    print(\"Batch cancelled: \", batch_id)\n",
    "\n",
    "def check_batch(batch_id):\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(\"Batch: \", batch)\n",
    "\n",
    "def retrieve_batch(file_id, save_path):\n",
    "    file_response = client.files.content(file_id)\n",
    "    response_text = file_response.text\n",
    "\n",
    "    responses = [json.loads(line) for line in response_text.strip().split('\\n')]\n",
    "    output_path = save_path\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        json.dump(responses, outfile, indent=2)\n",
    "\n",
    "    print(f\"Responses saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76e500",
   "metadata": {},
   "source": [
    "## Check and retrieve batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit_batch(out_path, \"Rollout requests for GPT-4o\")\n",
    "\n",
    "# Check the batch status\n",
    "\n",
    "# check_batch(\"batch_id\") # replace the batch id\n",
    "\n",
    "# Cancel the batch if needed\n",
    "# cancel_batch(batch_id) # replace the batch id\n",
    "\n",
    "\n",
    "# Retrieve batch responses\n",
    "\n",
    "output_file_path = \"./openai_rollout_responses.json\"\n",
    "retrieve_batch(\"output_file_id\", output_file_path) # replace output file id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c1b7f",
   "metadata": {},
   "source": [
    "## Process batch request output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff44219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Notebook-friendly parser for a *single* OpenAI Batch JSON/JSONL file.\n",
    "\n",
    "Features:\n",
    "- STRICT custom_id parsing via regex: ^(?P<base>.+?)__rep(?P<rep>\\d+)__\n",
    "  -> prevents accidental grouping like `multilogieval_136` with `multilogieval_1364`.\n",
    "- EXACTLY THREE samples per base id: keeps at most rep0, rep1, rep2 (in order).\n",
    "- DEDUP per rep: if duplicates exist for a rep, the first occurrence is kept.\n",
    "- Robust JSON/JSONL reading (handles arrays, JSONL, lines with trailing commas).\n",
    "\n",
    "Edit BATCH_PATH / GROUND_PATH / OUT_PATH below, then call `run()` (e.g. in a notebook).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# ====== EDIT THESE ======\n",
    "BATCH_PATH = output_file_path      \n",
    "GROUND_PATH = \"./data.jsonl\"    \n",
    "OUT_PATH = \"./openai_rollout_processed.jsonl\"      \n",
    "# ========================\n",
    "\n",
    "ANSWER_RE = re.compile(r\"<answer>\\s*(.*?)\\s*</answer>\", re.IGNORECASE | re.DOTALL)\n",
    "# Strictly capture base id and rep index\n",
    "CUSTOM_ID_RE = re.compile(r\"^(?P<base>.+?)__rep(?P<rep>\\d+)__\")\n",
    "ALLOWED_REPS = (0, 1, 2)\n",
    "\n",
    "\n",
    "def _json_loads_lenient(line: str) -> object:\n",
    "    \"\"\"Try to load a JSON object, trimming trailing commas if present.\"\"\"\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except json.JSONDecodeError:\n",
    "        trimmed = line.rstrip().rstrip(\",\")\n",
    "        if trimmed != line:\n",
    "            try:\n",
    "                return json.loads(trimmed)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_json_or_jsonl(path: Path) -> Iterable[dict]:\n",
    "    \"\"\"\n",
    "    Read a single file that may be:\n",
    "      - JSONL (one JSON object per line; sometimes lines end with commas)\n",
    "      - JSON array of objects\n",
    "      - Single JSON object (wrapped into an iterable of one)\n",
    "    Yields dict items.\n",
    "    \"\"\"\n",
    "    text = path.read_text(encoding=\"utf-8\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Try full JSON first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                if isinstance(item, dict):\n",
    "                    yield item\n",
    "                else:\n",
    "                    yield {\"value\": item}\n",
    "            return\n",
    "        if isinstance(obj, dict):\n",
    "            yield obj\n",
    "            return\n",
    "        # Fall through to JSONL if other types\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Treat as JSONL\n",
    "    for raw in text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            item = _json_loads_lenient(line)\n",
    "            if isinstance(item, dict):\n",
    "                yield item\n",
    "            else:\n",
    "                yield {\"value\": item}\n",
    "        except json.JSONDecodeError:\n",
    "            # Ignore malformed fragments\n",
    "            continue\n",
    "\n",
    "\n",
    "def load_ground_truth(ground_truth_path: Path) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Load ground-truth JSONL: returns mapping base_id -> record.\n",
    "    Each record is expected to include: id, premise, question, answer.\n",
    "    \"\"\"\n",
    "    gt: Dict[str, dict] = {}\n",
    "    with ground_truth_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[warn] ground-truth line {i} JSON error: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "            rec_id = rec.get(\"id\")\n",
    "            if not rec_id:\n",
    "                print(f\"[warn] ground-truth line {i} missing 'id'\", file=sys.stderr)\n",
    "                continue\n",
    "            gt[rec_id] = rec\n",
    "    return gt\n",
    "\n",
    "\n",
    "def parse_custom_id(custom_id: str) -> Optional[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Extract (base_id, rep_idx) using a strict regex. Returns None if not matched.\n",
    "\n",
    "    Example:\n",
    "      'multilogieval_136__rep0__gpt-4o-2024-01-01' -> ('multilogieval_136', 0)\n",
    "    \"\"\"\n",
    "    match = CUSTOM_ID_RE.match(custom_id)\n",
    "    if not match:\n",
    "        return None\n",
    "    base = match.group(\"base\")\n",
    "    rep_s = match.group(\"rep\")\n",
    "    try:\n",
    "        rep = int(rep_s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return base, rep\n",
    "\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Return content inside <answer>...</answer>, or '' if not found.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    match = ANSWER_RE.search(text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def pull_generation_from_entry(entry: dict) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the assistant content from a batch entry.\n",
    "\n",
    "    Handles both:\n",
    "      - entry[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "      - entry[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    with a fallback to \"text\" if present.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        body = entry.get(\"response\", {}).get(\"body\", {})\n",
    "        choices = body.get(\"choices\")\n",
    "        if isinstance(choices, list) and choices:\n",
    "            msg = choices[0].get(\"message\") or {}\n",
    "            content = msg.get(\"content\")\n",
    "            if isinstance(content, str):\n",
    "                return content\n",
    "\n",
    "        # Alternative layout: entry[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        choices_alt = entry.get(\"body\", {}).get(\"choices\")\n",
    "        if isinstance(choices_alt, list) and choices_alt:\n",
    "            msg = choices_alt[0].get(\"message\") or {}\n",
    "            content = msg.get(\"content\")\n",
    "            if isinstance(content, str):\n",
    "                return content\n",
    "\n",
    "        # Fallback: choices[0][\"text\"]\n",
    "        if isinstance(choices, list) and choices:\n",
    "            text = choices[0].get(\"text\")\n",
    "            if isinstance(text, str):\n",
    "                return text\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not extract generation: {e}\", file=sys.stderr)\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_batch(batch_path: Path) -> Dict[str, Dict[int, Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Parse the batch output file.\n",
    "\n",
    "    Returns:\n",
    "      base_id -> { rep_idx -> (generation_text, extracted_answer) }\n",
    "\n",
    "    Only keeps rep indices in ALLOWED_REPS and the *first* occurrence per rep.\n",
    "    \"\"\"\n",
    "    groups: Dict[str, Dict[int, Tuple[str, str]]] = {}\n",
    "\n",
    "    for entry in read_json_or_jsonl(batch_path):\n",
    "        custom_id = entry.get(\"custom_id\")\n",
    "        if not custom_id:\n",
    "            print(\"[warn] missing custom_id in entry; skipping.\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        parsed = parse_custom_id(custom_id)\n",
    "        if not parsed:\n",
    "            # Ignore entries that do not match the strict pattern\n",
    "            continue\n",
    "\n",
    "        base_id, rep_idx = parsed\n",
    "\n",
    "        if rep_idx not in ALLOWED_REPS:\n",
    "            # Ignore reps beyond the sampled set (keep exactly 0, 1, 2)\n",
    "            continue\n",
    "\n",
    "        gen = pull_generation_from_entry(entry)\n",
    "        if gen is None:\n",
    "            print(f\"[warn] no generation content for {custom_id}; skipping.\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        ans = extract_answer(gen)\n",
    "\n",
    "        # Create base bucket\n",
    "        if base_id not in groups:\n",
    "            groups[base_id] = {}\n",
    "\n",
    "        # Dedup per rep: keep first occurrence only\n",
    "        if rep_idx in groups[base_id]:\n",
    "            continue\n",
    "\n",
    "        groups[base_id][rep_idx] = (gen, ans)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def run() -> str:\n",
    "    \"\"\"Parse batch outputs, join with ground truth, and write a processed JSONL.\"\"\"\n",
    "    batch_path = Path(BATCH_PATH)\n",
    "    if not batch_path.exists():\n",
    "        print(f\"[error] batch file not found: {batch_path}\", file=sys.stderr)\n",
    "        return \"\"\n",
    "\n",
    "    ground_path = Path(GROUND_PATH)\n",
    "    if not ground_path.exists():\n",
    "        print(f\"[error] ground-truth file not found: {ground_path}\", file=sys.stderr)\n",
    "        return \"\"\n",
    "\n",
    "    out_path = Path(OUT_PATH)\n",
    "\n",
    "    gt = load_ground_truth(ground_path)\n",
    "    groups = parse_batch(batch_path)\n",
    "\n",
    "    total_written = 0\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as out_f:\n",
    "        for base_id, rep_map in groups.items():\n",
    "            gt_rec = gt.get(base_id)\n",
    "            if not gt_rec:\n",
    "                # Strict exact match with ground-truth id only\n",
    "                print(\n",
    "                    f\"[warn] base_id '{base_id}' not found in ground truth; skipping.\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Assemble in rep order 0,1,2 (skip missing)\n",
    "            generations: List[str] = []\n",
    "            extracted: List[str] = []\n",
    "            for r in ALLOWED_REPS:\n",
    "                if r in rep_map:\n",
    "                    gen, ans = rep_map[r]\n",
    "                    generations.append(gen)\n",
    "                    extracted.append(ans)\n",
    "\n",
    "            out_obj = {\n",
    "                \"id\": base_id,\n",
    "                \"premise\": gt_rec.get(\"premise\"),\n",
    "                \"question\": gt_rec.get(\"question\"),\n",
    "                \"answer\": gt_rec.get(\"answer\"),\n",
    "                \"generation\": generations,\n",
    "                \"extracted_answer\": extracted,\n",
    "            }\n",
    "            out_f.write(json.dumps(out_obj, ensure_ascii=False) + \"\\n\")\n",
    "            total_written += 1\n",
    "\n",
    "    print(f\"[info] wrote {total_written} records to {out_path}\")\n",
    "    return str(out_path)\n",
    "\n",
    "\n",
    "run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aristotle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
