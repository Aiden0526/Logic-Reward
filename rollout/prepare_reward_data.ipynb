{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfb2725",
   "metadata": {},
   "source": [
    "## Merge openai and qwen rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d25383",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge openai and qwen\n",
    "\n",
    "file1 = \"./openai_rollout_processed.jsonl\"   # first file\n",
    "file2 = \"./qwen_rollout.jsonl\"     # second file\n",
    "out_file = \"./rollout_full.jsonl\"  # output\n",
    "\n",
    "count = 0\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for src in (file1, file2):\n",
    "        with open(src, \"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                if line.strip():  # skip blank lines\n",
    "                    fout.write(line + \"\\n\")\n",
    "                    count += 1\n",
    "\n",
    "print(f\"Done. Wrote {count} total records to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b74c0",
   "metadata": {},
   "source": [
    "## Post process for reward later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Parse step-by-step reasoning generations into a structured JSONL format.\n",
    "\n",
    "Input\n",
    "-----\n",
    "Each line in the input JSONL is expected to be either:\n",
    "  - A dict with keys:\n",
    "      \"id\", \"premise\", \"question\", \"answer\",\n",
    "      \"generation\" (list[str]) or \"generations\" (list[str])\n",
    "  - A list of such dicts (less common)\n",
    "\n",
    "Each `generation` is a text blob that (ideally) follows a pattern like:\n",
    "\n",
    "    Step 1:\n",
    "    Premises:\n",
    "    - ...\n",
    "    Assumptions:\n",
    "    - ...\n",
    "    Conclusion:\n",
    "    - ...\n",
    "\n",
    "    Step 2:\n",
    "    ...\n",
    "\n",
    "    Final:\n",
    "    <answer>LABEL</answer>\n",
    "\n",
    "Output\n",
    "------\n",
    "For each input record, we write one line to the output JSONL with the schema:\n",
    "\n",
    "{\n",
    "  \"id\": ...,\n",
    "  \"premise\": ...,\n",
    "  \"question\": ...,\n",
    "  \"answer\": ...,\n",
    "  \"extracted_answer\": [... or None],\n",
    "  \"generations\": [\n",
    "    {\n",
    "      \"final_tag\": str | null,\n",
    "      \"step_order_ok\": bool,\n",
    "      \"step_numbers\": [int],\n",
    "      \"missing_steps\": [int],\n",
    "      \"duplicate_steps\": [int],\n",
    "      \"steps\": [\n",
    "        {\n",
    "          \"n\": int,\n",
    "          \"premises\": [str],\n",
    "          \"assumptions\": [str],\n",
    "          \"conclusion\": str\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Step / section parsing\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "STEP_RE = re.compile(r\"(?:^|\\n)Step\\s+(\\d+)\\s*:?\\s*\\n\", re.IGNORECASE)\n",
    "FINAL_RE = re.compile(r\"(?:^|\\n)Final\\s*:?\", re.IGNORECASE)\n",
    "\n",
    "SECTION_PATTERNS = {\n",
    "    \"premises\": re.compile(r\"(?:^|\\n)\\s*Premises?\\s*:\\s*\", re.IGNORECASE),\n",
    "    \"assumptions\": re.compile(r\"(?:^|\\n)\\s*Assumptions?\\s*:\\s*\", re.IGNORECASE),\n",
    "    \"conclusion\": re.compile(r\"(?:^|\\n)\\s*Conclusion\\s*:\\s*\", re.IGNORECASE),\n",
    "}\n",
    "\n",
    "ANSWER_TAG_RE = re.compile(\n",
    "    r\"<\\s*answer\\s*>\\s*(.*?)\\s*<\\s*/\\s*answer\\s*>\",\n",
    "    re.IGNORECASE | re.DOTALL,\n",
    ")\n",
    "\n",
    "\n",
    "def _clean_bullets(block: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Turn a text block into a list of cleaned lines:\n",
    "    - strip leading/trailing whitespace\n",
    "    - remove leading bullet characters like \"- \" or \"• \"\n",
    "    - drop empty lines\n",
    "    \"\"\"\n",
    "    items: List[str] = []\n",
    "    for line in block.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Remove leading bullet characters like \"- \" or \"• \"\n",
    "        line = re.sub(r\"^[\\-\\u2022]\\s*\", \"\", line)\n",
    "        items.append(line)\n",
    "    return items\n",
    "\n",
    "\n",
    "def _extract_sections(step_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract sections for a single step.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "      - \"premises\": List[str]\n",
    "      - \"assumptions\": List[str]\n",
    "      - \"conclusion\": str\n",
    "\n",
    "    Missing sections yield [] or \"\" respectively.\n",
    "    \"\"\"\n",
    "    found = []\n",
    "    for key, pat in SECTION_PATTERNS.items():\n",
    "        m = pat.search(step_text)\n",
    "        if m:\n",
    "            found.append((key, m.start(), m.end()))\n",
    "\n",
    "    # If no known headers, return empty\n",
    "    if not found:\n",
    "        return {\"premises\": [], \"assumptions\": [], \"conclusion\": \"\"}\n",
    "\n",
    "    # Sort by position to slice content between headers\n",
    "    found.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Build a dict of raw blocks between headers\n",
    "    raw_blocks: Dict[str, str] = {}\n",
    "    for i, (key, _start, end) in enumerate(found):\n",
    "        next_start = len(step_text) if i == len(found) - 1 else found[i + 1][1]\n",
    "        raw_blocks[key] = step_text[end:next_start].strip()\n",
    "\n",
    "    # Clean and normalize\n",
    "    premises_list = _clean_bullets(raw_blocks.get(\"premises\", \"\")) if \"premises\" in raw_blocks else []\n",
    "    assumptions_list = _clean_bullets(raw_blocks.get(\"assumptions\", \"\")) if \"assumptions\" in raw_blocks else []\n",
    "    conclusion_lines = _clean_bullets(raw_blocks.get(\"conclusion\", \"\")) if \"conclusion\" in raw_blocks else []\n",
    "    conclusion = \" \".join(conclusion_lines).strip()\n",
    "\n",
    "    return {\n",
    "        \"premises\": premises_list,\n",
    "        \"assumptions\": assumptions_list,\n",
    "        \"conclusion\": conclusion,\n",
    "    }\n",
    "\n",
    "\n",
    "def _find_step_blocks(text: str) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Return a list of (step_number, step_text_block) in order.\n",
    "\n",
    "    Splits on 'Step N:' and stops each block at the next 'Step M:', 'Final:',\n",
    "    or the end of the text.\n",
    "    \"\"\"\n",
    "    steps = list(STEP_RE.finditer(text))\n",
    "    if not steps:\n",
    "        return []\n",
    "\n",
    "    # Find 'Final:' marker (if any) to cap the last step\n",
    "    final_match = FINAL_RE.search(text)\n",
    "    final_start = final_match.start() if final_match else len(text)\n",
    "\n",
    "    blocks: List[Tuple[int, str]] = []\n",
    "    for i, m in enumerate(steps):\n",
    "        step_num = int(m.group(1))\n",
    "        start = m.end()\n",
    "        if i + 1 < len(steps):\n",
    "            end = steps[i + 1].start()\n",
    "        else:\n",
    "            end = final_start  # stop at \"Final:\" if present; else end of text\n",
    "        block = text[start:end].strip()\n",
    "        if block:\n",
    "            blocks.append((step_num, block))\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Transformation helpers for the target output format\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def parse_single_generation(gen_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse one generation text into a structured object:\n",
    "\n",
    "    {\n",
    "      \"final_tag\": str | None,\n",
    "      \"step_order_ok\": bool,\n",
    "      \"step_numbers\": [int],\n",
    "      \"missing_steps\": [int],\n",
    "      \"duplicate_steps\": [int],\n",
    "      \"steps\": [\n",
    "        {\n",
    "          \"n\": int,\n",
    "          \"premises\": [...],\n",
    "          \"assumptions\": [...],\n",
    "          \"conclusion\": str\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    # 1) Steps\n",
    "    steps_info: List[Dict[str, Any]] = []\n",
    "    for step_num, step_block in _find_step_blocks(gen_text):\n",
    "        sections = _extract_sections(step_block)\n",
    "        steps_info.append(\n",
    "            {\n",
    "                \"n\": step_num,\n",
    "                \"premises\": sections[\"premises\"],\n",
    "                \"assumptions\": sections[\"assumptions\"],\n",
    "                \"conclusion\": sections[\"conclusion\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 2) Step numbers & order checks\n",
    "    step_numbers = [s[\"n\"] for s in steps_info]\n",
    "    step_order_ok = step_numbers == sorted(step_numbers)\n",
    "\n",
    "    # Missing steps: assume steps should start at 1 if any exist\n",
    "    missing_steps: List[int] = []\n",
    "    if step_numbers:\n",
    "        expected = set(range(1, max(step_numbers) + 1))\n",
    "        present = set(step_numbers)\n",
    "        missing_steps = sorted(list(expected - present))\n",
    "\n",
    "    # Duplicates\n",
    "    counts: Dict[int, int] = {}\n",
    "    for n in step_numbers:\n",
    "        counts[n] = counts.get(n, 0) + 1\n",
    "    duplicate_steps = sorted([n for n, c in counts.items() if c > 1])\n",
    "\n",
    "    # 3) Final tag (from <answer>...</answer>)\n",
    "    match = ANSWER_TAG_RE.search(gen_text)\n",
    "    final_tag: Optional[str] = match.group(1).strip() if match else None\n",
    "\n",
    "    return {\n",
    "        \"final_tag\": final_tag,\n",
    "        \"step_order_ok\": bool(step_order_ok),\n",
    "        \"step_numbers\": step_numbers,\n",
    "        \"missing_steps\": missing_steps,\n",
    "        \"duplicate_steps\": duplicate_steps,\n",
    "        \"steps\": sorted(steps_info, key=lambda d: d[\"n\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_record(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transform an input JSON record into the target output schema.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"premise\": data.get(\"premise\"),\n",
    "        \"question\": data.get(\"question\"),\n",
    "        \"answer\": data.get(\"answer\"),\n",
    "        \"extracted_answer\": data.get(\"extracted_answer\"),\n",
    "        \"generations\": [],\n",
    "    }\n",
    "\n",
    "    # The input may use key \"generation\" (list of strings) or \"generations\"\n",
    "    generations = data.get(\"generation\") or data.get(\"generations\") or []\n",
    "    for gen_text in generations:\n",
    "        out[\"generations\"].append(parse_single_generation(gen_text))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # Edit these paths as needed\n",
    "    input_path = \"./rollout_full.jsonl\"\n",
    "    output_path = \"./rollout_processed.jsonl\"\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f_in, open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f_out:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            raw = json.loads(line)\n",
    "\n",
    "            if isinstance(raw, dict) and \"generation\" in raw:\n",
    "                result: Any = transform_record(raw)\n",
    "            elif isinstance(raw, list):\n",
    "                result = [transform_record(rec) for rec in raw]\n",
    "            else:\n",
    "                # Fallback: treat as a single dict-shaped record\n",
    "                result = transform_record(raw)\n",
    "\n",
    "            f_out.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote output to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aristotle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
